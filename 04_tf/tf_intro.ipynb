{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Официальная документация](https://www.tensorflow.org)\n",
    "* [Get started](https://www.tensorflow.org/get_started/get_started)\n",
    "* [Models](https://github.com/tensorflow/models/)\n",
    "\n",
    "Из-за быстроты развития лучше всего смотреть на официальную документацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть два типа установки: для CPU и GPU. Для CPU устанавливается стандартно через `pip`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для GPU немного сложнее \n",
    "* проверить совместимость с видеокартой. Параметр CUDA Compute Capability должен быть больше 3.0\n",
    "* Установить CUDA Toolkit восьмой версии\n",
    "* Установить cuDNN версии 5.1\n",
    "* Установить из pip пакет tensorflow-gpu\n",
    "\n",
    "Можно установить через Docker. Можно и из исходников (говорят, что так рабоатет быстрее, потому что он не тянет кучу плюшек к себе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовые элементы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой код, с помощью которого легко убедиться, что все работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # подключаем TF\n",
    "hello = tf.constant('Hello, TensorFlow!') # создаем объект из TF\n",
    "sess = tf.InteractiveSession() # создаем сессию\n",
    "print(sess.run(hello)) #сессия \"выполняет\" объект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Граф"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся работа с TF строится вокруг построения графа вычислений. Есть гарф - есть программа. По сути - это описание того, как будут проводиться вычисления. Основа TF - создать структуру, которая задаст порядок вычислений. Соответственно программа это: \n",
    "* составление графа вычислений \n",
    "* выполнение вычислений в структурах. \n",
    "\n",
    "Составляющие графа:\n",
    "* Плейсхолдеры\n",
    "* Переменные\n",
    "* Операции\n",
    "Из этого собираем граф, в котором будут вычисляться тензоры. **Тензоры** это многомерные массивы, по своей сути это топливо графа.Тензором может быть как отдельное число, вектор, так и целый батч.Вместо одного объекта можно передать массив объектов - на выходе получить массив ответов. По сути это обработка массива в numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение графа происходит в сессии (*tf.Session()*). Такой объект скрывает в себе контекст выполнения графа (ресурсы, классы, адресные пространства). Два типа сессии:\n",
    "* Обычные (**tf.Session()**)\n",
    "* Интерактивные (**tf.InteractiveSession()**)\n",
    "Интерактивная для консоли. Основной эффект — объект сессии не нужно передавать в функции вычисления как параметр.\n",
    "\n",
    "Tensorboard генерирует графы за вас на примере:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тензоры, операции, переменные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с нулями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeros_tensor = tf.zeros([3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы посмотреть тензор, необходимо выполнить его. Пока без графа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Tensor(\"zeros:0\", shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(zeros_tensor.eval())\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различия: в первой строке вычисление, во второй представление объекта. Что дает описание:\n",
    "* Имя тензора\n",
    "* Форма тензора (размерность массива numpy)\n",
    "* Типизация тензора\n",
    "\n",
    "Есть множество операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45460051  0.46388394]\n",
      " [-0.28661668  1.51374352]]\n",
      "[[-1.01343989 -1.01343989]\n",
      " [-0.17550369 -0.17550369]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.truncated_normal([2, 2]) #усеченое нормальное распределение\n",
    "b = tf.fill([2, 2], 0.5) #массив из 0.5 \n",
    "print(sess.run(a + b)) \n",
    "print(sess.run(tf.matmul(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sess.run()** метод исполнения операций в графе. \n",
    "\n",
    "Создадим переменную на основе тензора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.Variable(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему нельзя просто задать? Так как переменная будет в качестве узла графа. Есть еще плейсхолдеры - параметризуют граф и отмечают места для постановки нужных значений (внешних) по сути - обещание поставить значение потом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = (4, 4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(\"float\")\n",
    "b = tf.placeholder(\"float\")\n",
    "y = tf.multiply(a, b)\n",
    "print(sess.run(y, feed_dict={a:100, b:500}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "f = 1 + 2 * x + tf.pow(x, 2)\n",
    "sess.run(f, feed_dict={x:10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00669285,  0.00819568,  0.01003255,  0.01227603,  0.01501357,\n",
       "        0.01835024,  0.02241159,  0.02734679,  0.03333168,  0.04057176,\n",
       "        0.04930425,  0.05979915,  0.07235796,  0.0873094 ,  0.10500059,\n",
       "        0.12578245,  0.14998817,  0.17790413,  0.20973383,  0.24555731,\n",
       "        0.28529069,  0.32865256,  0.3751457 ,  0.42406148,  0.47451192,\n",
       "        0.52548808,  0.57593852,  0.62485433,  0.67134744,  0.71470934,\n",
       "        0.75444269,  0.79026616,  0.82209587,  0.85001183,  0.87421757,\n",
       "        0.89499938,  0.91269064,  0.92764211,  0.94020081,  0.95069569,\n",
       "        0.95942819,  0.96666825,  0.97265327,  0.97758842,  0.98164982,\n",
       "        0.98498636,  0.98772395,  0.98996747,  0.99180436,  0.99330717], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "sigma = 1 / (1 + tf.exp(-x))\n",
    "sigma.eval(feed_dict={x: np.linspace(-5, 5) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В фрагменте с запуском вычисления функции есть один момент, который отличает этот пример от предыдущих. Дело в том, что в плейсхолдер вместо одного скалярного значения мы передаем целый массив. TF обрабатывает все значения массива вместе, в рамках одного тензора (помним, что массив == тензор). Точно таким же образом мы можем передавать в граф объекты целыми батчами и поставлять нейронной сети картинки целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение и загрузка графов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть специальный объект-сериализатор, который делает две вещи:\n",
    "1. Сохраняет текущий граф, состояние и значения в файл\n",
    "2. Читает то же самое из файла\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value Variable\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_8/Const_0, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, Variable)]]\n\nCaused by op 'save_8/SaveV2', defined at:\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-2358f558c33b>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 689, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 219, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 780, in save_v2\n    tensors=tensors, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_8/Const_0, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, Variable)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_8/Const_0, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, Variable)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-2358f558c33b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"test.ckpt\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1389\u001b[0m       model_checkpoint_path = sess.run(\n\u001b[1;32m   1390\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1391\u001b[0;31m           {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1392\u001b[0m       \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value Variable\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_8/Const_0, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, Variable)]]\n\nCaused by op 'save_8/SaveV2', defined at:\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2683, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2787, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2847, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-35-2358f558c33b>\", line 1, in <module>\n    saver = tf.train.Saver()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 689, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 276, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 219, in save_op\n    tensors)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 780, in save_v2\n    tensors=tensors, name=name)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value Variable\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save_8/Const_0, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, Variable)]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '\"test.ckpt\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ckpt_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-b66c7e0c8963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_checkpoint_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ckpt_dir' is not defined"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Крайне полезная система в составе TF — web-dashboard, который позволяет собирать статистику из дампов и логов и наблюдать, что же всё-таки происходит во время вычислений. Крайне удобно то, что дашборд работает на веб-сервере и можно, например, запустив tensorboard на удаленной машине в облаке, наблюдать происходящее у себя в окне браузера.\n",
    "\n",
    "Tensorboard умеет:\n",
    "\n",
    "1. Рисовать граф вычислений.\n",
    "2. Граф вычислений стоит посмотреть хотя бы для самопроверки, чтобы убедиться в том, что собралось и считается именно то, что планировалось, и при кодировании не допущено ошибок.\n",
    "3. Показывать статистику по переменным.\n",
    "4. Можно собирать вообще любую статистику.\n",
    "5. Есть средство для анализа многомерных данных (например, эмбеддингов). Для этого в дашборде встроены PCA и t-SNE, с которыми можно попробовать рассмотреть данные в 2 и 3 измерениях.\n",
    "6. Гистограммы. Можно строить гистограммы распределений выходов слоев сетей и поведения переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратная сторона медали — чтобы статистика попадала в дашборд, её нужно сохранять в логи (в формате protobuf) с помощью специального API. API не очень сложный, сгруппирован в **tf.summary**. Для сбора статистики нужно будет отдельно зарегистрировать интересующие переменные с помощью специальных функций и потом отдельно сохранить всё в лог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_output:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.histogram(\"layer_output\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.summary.scalar(\"accuracy\", learning_rate)**\n",
    "\n",
    "Сохранять логи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs/nn_logs\", sess.graph) # for 1.0\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простого: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "merged = tf.summary.merge_all(key='summaries')\n",
    "if not os.path.exists('tensorboard_logs/'):\n",
    "    os.makedirs('tensorboard_logs/')\n",
    "my_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию Tensorboard локально доступен по адресу 127.0.1.1:6006. **`tensorboard --logdir=path/to/log-directory`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache beam and tf tranform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF: https://github.com/tensorflow/transform\n",
    "* Beam: https://beam.apache.org/documentation/programming-guide/\n",
    "\n",
    "**Apache Beam**.Это набор интерфейсов для создания data processing pipeline. Вы пишете программу с помощью этих интерфейсов, а потом запускаете ее на конкретном движке, будь это Apache Spark или Google Cloud DataFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam только для Python 2.7"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install tensorflow-transform\n",
    "!pip install apache-beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tr.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.Transform** библиотека для выполнения предварительной обработки данных с TensorFlow. Он позволяет комбинировать различные фреймворки обработки данных (Apache Beam). Удобно, что можно включать это в сам граф Tensorflow (можно увидеть сколько занимал препроцессинг данных, например). Или можно экспортировать граф обработки данных, чтобы препроцессинг и обучение модели были более менее независимыми. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что TF позволяет работать с батчами данных, для некоторых операций необходим полный проход по датасету.Например, нормализация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определение функции Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем, как определить «preprocessing function», которая является логическим описанием pipline, который преобразует исходные данные в данные, которые будут использоваться для обучения модели ML. \n",
    "\n",
    "Набор данных - словарь столбцов, а функция предварительной обработки определяется двумя основными механизмами:\n",
    "1. **tf.map**: принимает определенную пользователем функцию, которая принимает и возвращает тензоры. Такая функция может использовать любую операцию TensorFlow для построения выходных тензоров (от входных). Остальные аргументы - столбцы, к которым применяется функция. Количество столбцов = количеству аргументов функции. Аналог `map` в Python. Каждая строка обрабатывается независимо друг от друга.\n",
    "2. **analyzers**: анализаторы являются функциями, которые принимают один или несколько колонок и ворзвращает некоторые сводные статистические данные для входного столбца или столбцов. Пример анализатора **tft.min**.\n",
    "\n",
    "Объединив анализаторы и tft.map пользователи могут гибко создавать pipline для преобразования данных. Следующая функция предварительной обработки преобразует каждый из трех колонок по-разному."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  x = inputs['x']\n",
    "  y = inputs['y']\n",
    "  s = inputs['s']\n",
    "  x_centered = tft.map(lambda x, mean: x - mean, x, tft.mean(x))\n",
    "  y_normalized = tft.scale_to_0_1(y)\n",
    "  s_integerized = tft.string_to_int(s)\n",
    "  x_centered_times_y_normalized = tft.map(lambda x, y: x * y,\n",
    "                                          x_centered, y_normalized)\n",
    "  return {\n",
    "      'x_centered': x_centered,\n",
    "      'y_normalized': y_normalized,\n",
    "      'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "      's_integerized': s_integerized\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* `x, y, s` входные столбцы\n",
    "* Первый новый столбец **x_centered**, строится путем составления `tft.map и tft.mean`. **tft.mean(x)** возвращает статистическую величину, представляющую среднее значение столбца x.\n",
    "* Второй новый столбец **y_normalized**, созданный таким же образом, но с использованием методы **tft.scale_to_0_1**.\n",
    "* Колонка **s_integerized** показывает пример работы со строками. В этом простом случае мы берем строку и переводим в целое число.\n",
    "* **x_centered_times_y_normalized** объединенная колонка.\n",
    "\n",
    "Типичные рабочий процесс пользователя tf.Transform будет построить функцию предварительной обработки, а затем включить это в больший pipline Beam (материализует данные для обучения).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя это не очевидно из приведенного выше примера, заданная пользовательская функция передается tft.map будет передаваться тензорами, представляющих батчи, а не отдельные экземпляры, так же, как будет происходить во время подготовки TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация Canonical Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.Transform** обеспечивает каноническую реализацию препроцессинга данных, которая запускает функцию предварительной обработки на Apache Beam."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "raw_data = [\n",
    "    {'x': 1, 'y': 1, 's': 'hello'},\n",
    "    {'x': 2, 'y': 2, 's': 'world'},\n",
    "    {'x': 3, 'y': 3, 's': 'hello'}\n",
    "]\n",
    "\n",
    "raw_data_metadata = ...\n",
    "transformed_dataset, transform_fn = (\n",
    "    (raw_data, raw_data_metadata) | beam_impl.AnalyzeAndTransformDataset(\n",
    "        preprocessing_fn, tempfile.mkdtemp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Содержание **transformed_dataset** будет показано ниже, он содержать преобразованные столбцы в том же формате, что и исходные данные. В частности, значения s_integerized = `[0, 1, 0]`(эти значения зависят от того, как слова hello и world были нанесены на карту в целые числа, который является детерминированным). Для столбца x_centered мы вычитали среднее значение, поэтому значения столбца x,  `[1.0, 2.0, 3.0]` стали `[-1.0, 0.0, 1.0]`. Точно так же все остальные столбцы соответствуют их ожидаемым значениям."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[{u's_integerized': 0,\n",
    "  u'x_centered': -1.0,\n",
    "  u'x_centered_times_y_normalized': -0.0,\n",
    "  u'y_normalized': 0.0},\n",
    " {u's_integerized': 1,\n",
    "  u'x_centered': 0.0,\n",
    "  u'x_centered_times_y_normalized': 0.0,\n",
    "  u'y_normalized': 0.5},\n",
    " {u's_integerized': 0,\n",
    "  u'x_centered': 1.0,\n",
    "  u'x_centered_times_y_normalized': 1.0,\n",
    "  u'y_normalized': 1.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Оба **raw_data** и **transformed_data** являются наборами данных. Другое возвращаемое значение, **transform_fn** является представлением преобразования, которое было сделано с данными (которые мы обсудим более подробно ниже).\n",
    "\n",
    "В самом деле, **AnalyzeAndTransformDataset** представляет собой композицию из двух фундаментальных преобразований , предусмотренных реализации, **AnalyzeDataset** и **TransformDataset**. То есть, эти два фрагмента кода ниже эквивалентны."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transformed_data, transform_fn = (\n",
    "    my_data | AnalyzeAndTransformDataset(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "transform_fn = my_data | AnalyzeDataset(preprocessing_fn)\n",
    "transformed_data = (my_data, transform_fn) | TransformDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_fn** является чистой функцией, которая представляет собой операцию, применяемая к каждой строке набора данных. В частности, все значения анализатора уже вычислены и рассматриваются как константы. В нашем примере, transform_fn будет содержать в качестве констант среднее значение столбца x, min/max колонны y и словарь, используемый для отображения строки в целые числа.\n",
    "\n",
    "\n",
    "Ключевая особенность tf.Transform является то , что transform_fnпредставляет собой map над rows (чистая функция, применяемая к каждой строке отдельно). Все вычисления с участием агрегирования строк делается **AnalyzeDataset**. Кроме того, transform_fn представляются в виде TensorFlow Graph, что означает, что он может быть встроен в обслуживающий граф.\n",
    "\n",
    "Google предлагает **AnalyzeAndTransformDataset** для того, чтобы обеспечить оптимизацию. Это аналоги scikit-learn, который обеспечивает fit, transformи fit_transform методы для препроцессоров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Форматы данных и схемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы упустили код для **raw_data_metadata**. В метаданных храниться схема, которая определяет расположение данных таким образом, что она может быть записана и прочитана в различные форматы (описание ниже)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n",
    "    's': dataset_schema.ColumnSchema(tf.string, [],\n",
    "        dataset_schema.FixedColumnRepresentation()),\n",
    "    'y': dataset_schema.ColumnSchema(tf.float32, [],\n",
    "        dataset_schema.FixedColumnRepresentation()),\n",
    "    'x': dataset_schema.ColumnSchema(tf.float32, [],\n",
    "        dataset_schema.FixedColumnRepresentation())\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **dataset_schema.Schema** класс представляет собой оболочку вокруг Dict из **dataset_schema.ColumnSchema**. Каждый ключ в cловаре описывает имя тензора, ColumnSchema описывает вид тензора и как он представлен в памяти или на диске.\n",
    "2. Первый аргумент **ColumnSchema** определяет, Domain который включает в себя тип данных и некоторые детали, такие как диапазоны. В нашем случае мы указали только тип данных.\n",
    "3. Второй аргумент содержит список **axis** объектов, которые описывают форму тензора. В нашем случае нет осей, так как это скаляры (rank 0 tensor).\n",
    "4. Третий аргумент является представлением данных. Есть три вида представления: \n",
    "    *  **FixedColumnRepresentation** является представлением колонки с фиксированным, известного размером.\n",
    "    * **ListColumnRepresentation** представляет колонки с разным размером\n",
    "    * **SparseColumnRepresentation** колонки с фиксированным размером (sparse  представление).\n",
    "    * подробнее [tf_metadata/dataset_schema.py](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_schema.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### IO with BEAM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Строим схему данных\n",
    "2. `csv_coder.CsvCoder`. Используем схему для чтения данных .csv. `ordered_columns` содержит названия всех колонок в порядке их появления в файле (сама схема не содержит эту информацию). \n",
    "3. Далее производим чтение из файла >> производим Map - декодирование"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "converter = csv_coder.CsvCoder(ordered_columns, raw_data_schema)\n",
    "\n",
    "raw_data = (\n",
    "    p\n",
    "    | 'ReadTrainData' >> textio.ReadFromText(train_data_file)\n",
    "    | ...\n",
    "    | 'DecodeTrainData' >> beam.Map(converter.decode))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Предварительная обработка затем переходит аналогично предыдущему примеру, за исключением того, что мы программно генерировали функцию предварительной обработки вместо того, чтобы вручную указать каждый столбец. Функция предварительной обработки показана ниже. NUMERICAL_COLUMNS и CATEGORICAL_COLUMNS списки, содержащие имена числовых и категориальных столбцов соответственно."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "  outputs = {}\n",
    "\n",
    "  # Scale numeric columns to have range [0, 1].\n",
    "  for key in NUMERIC_COLUMNS:\n",
    "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
    "\n",
    "  # For all categorical columns except the label column, we use\n",
    "  # tft.string_to_int which computes the set of unique values and uses this\n",
    "  # to convert the strings to indices.\n",
    "  for key in CATEGORICAL_COLUMNS:\n",
    "    outputs[key] = tft.string_to_int(inputs[key])\n",
    "\n",
    "  # For the label column we provide the mapping from string to index.\n",
    "  def convert_label(label):\n",
    "    table = lookup.string_to_index_table_from_tensor(['>50K', '<=50K'])\n",
    "    return table.lookup(label)\n",
    "  outputs[LABEL_COLUMN] = tft.map(convert_label, inputs[LABEL_COLUMN])\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**raw_data** Переменная представляет собой PCollection содержащий данные в том же формате, что и список raw_data из предыдущего примера, а также использование из AnalyzeAndTransformDataset преобразования одно и то же. Обратите внимание , что схема используется в двух местах: чтение данных из файла CSV, а также в качестве вклада AnalyzeAndTransformDataset. Это происходит потому , что как формат CSV и формат , в памяти должны быть соединены со схемой для того , чтобы интерпретировать их как тензоры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
