{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Официальная документация](https://www.tensorflow.org)\n",
    "* [Get started](https://www.tensorflow.org/get_started/get_started)\n",
    "* [Models](https://github.com/tensorflow/models/)\n",
    "\n",
    "Из-за быстроты развития лучше всего смотреть на официальную документацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть два типа установки: для CPU и GPU. Для CPU устанавливается стандартно через `pip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для GPU немного сложнее \n",
    "* проверить совместимость с видеокартой. Параметр CUDA Compute Capability должен быть больше 3.0\n",
    "* Установить CUDA Toolkit восьмой версии\n",
    "* Установить cuDNN версии 5.1\n",
    "* Установить из pip пакет tensorflow-gpu\n",
    "\n",
    "Можно установить через Docker. Можно и из исходников (говорят, что так рабоатет быстрее, потому что он не тянет кучу плюшек к себе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовые элементы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой код, с помощью которого легко убедиться, что все работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf # подключаем TF\n",
    "hello = tf.constant('Hello, TensorFlow!') # создаем объект из TF\n",
    "sess = tf.InteractiveSession() # создаем сессию\n",
    "print(sess.run(hello)) #сессия \"выполняет\" объект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Граф"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся работа с TF строится вокруг построения графа вычислений. Есть гарф - есть программа. По сути - это описание того, как будут проводиться вычисления. Основа TF - создать структуру, которая задаст порядок вычислений. Соответственно программа это: \n",
    "* составление графа вычислений \n",
    "* выполнение вычислений в структурах. \n",
    "\n",
    "Составляющие графа:\n",
    "* Плейсхолдеры\n",
    "* Переменные\n",
    "* Операции\n",
    "Из этого собираем граф, в котором будут вычисляться тензоры. **Тензоры** это многомерные массивы, по своей сути это топливо графа.Тензором может быть как отдельное число, вектор, так и целый батч.Вместо одного объекта можно передать массив объектов - на выходе получить массив ответов. По сути это обработка массива в numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение графа происходит в сессии (*tf.Session()*). Такой объект скрывает в себе контекст выполнения графа (ресурсы, классы, адресные пространства). Два типа сессии:\n",
    "* Обычные (**tf.Session()**)\n",
    "* Интерактивные (**tf.InteractiveSession()**)\n",
    "Интерактивная для консоли. Основной эффект — объект сессии не нужно передавать в функции вычисления как параметр.\n",
    "\n",
    "Tensorboard генерирует графы за вас на примере:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тензоры, операции, переменные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тензор с нулями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zeros_tensor = tf.zeros([3, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы посмотреть тензор, необходимо выполнить его. Пока без графа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n",
      "Tensor(\"zeros:0\", shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(zeros_tensor.eval())\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Различия: в первой строке вычисление, во второй представление объекта. Что дает описание:\n",
    "* Имя тензора\n",
    "* Форма тензора (размерность массива numpy)\n",
    "* Типизация тензора\n",
    "\n",
    "Есть множество операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45460051  0.46388394]\n",
      " [-0.28661668  1.51374352]]\n",
      "[[-1.01343989 -1.01343989]\n",
      " [-0.17550369 -0.17550369]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.truncated_normal([2, 2]) #усеченое нормальное распределение\n",
    "b = tf.fill([2, 2], 0.5) #массив из 0.5 \n",
    "print(sess.run(a + b)) \n",
    "print(sess.run(tf.matmul(a, b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** sess.run()** метод исполнения операций в графе. \n",
    "\n",
    "Создадим переменную на основе тензора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = tf.Variable(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему нельзя просто задать? Так как переменная будет в качестве узла графа. Есть еще плейсхолдеры - параметризуют граф и отмечают места для постановки нужных значений (внешних) по сути - обещание поставить значение потом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape = (4, 4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(\"float\")\n",
    "b = tf.placeholder(\"float\")\n",
    "y = tf.multiply(a, b)\n",
    "print(sess.run(y, feed_dict={a:100, b:500}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32)\n",
    "f = 1 + 2 * x + tf.pow(x, 2)\n",
    "sess.run(f, feed_dict={x:10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/2.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00669285,  0.00819568,  0.01003255,  0.01227603,  0.01501357,\n",
       "        0.01835024,  0.02241159,  0.02734679,  0.03333168,  0.04057176,\n",
       "        0.04930425,  0.05979915,  0.07235796,  0.0873094 ,  0.10500059,\n",
       "        0.12578245,  0.14998817,  0.17790413,  0.20973383,  0.24555731,\n",
       "        0.28529069,  0.32865256,  0.3751457 ,  0.42406148,  0.47451192,\n",
       "        0.52548808,  0.57593852,  0.62485433,  0.67134744,  0.71470934,\n",
       "        0.75444269,  0.79026616,  0.82209587,  0.85001183,  0.87421757,\n",
       "        0.89499938,  0.91269064,  0.92764211,  0.94020081,  0.95069569,\n",
       "        0.95942819,  0.96666825,  0.97265327,  0.97758842,  0.98164982,\n",
       "        0.98498636,  0.98772395,  0.98996747,  0.99180436,  0.99330717], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = tf.placeholder(dtype=tf.float32)\n",
    "sigma = 1 / (1 + tf.exp(-x))\n",
    "sigma.eval(feed_dict={x: np.linspace(-5, 5) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В фрагменте с запуском вычисления функции есть один момент, который отличает этот пример от предыдущих. Дело в том, что в плейсхолдер вместо одного скалярного значения мы передаем целый массив. TF обрабатывает все значения массива вместе, в рамках одного тензора (помним, что массив == тензор). Точно таким же образом мы можем передавать в граф объекты целыми батчами и поставлять нейронной сети картинки целиком."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение и загрузка графов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть специальный объект-сериализатор, который делает две вещи:\n",
    "1. Сохраняет текущий граф, состояние и значения в файл\n",
    "2. Читает то же самое из файла\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, '\"test.ckpt\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(ckpt_dir)\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print(ckpt.model_checkpoint_path)\n",
    "    saver.restore(session, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Крайне полезная система в составе TF — web-dashboard, который позволяет собирать статистику из дампов и логов и наблюдать, что же всё-таки происходит во время вычислений. Крайне удобно то, что дашборд работает на веб-сервере и можно, например, запустив tensorboard на удаленной машине в облаке, наблюдать происходящее у себя в окне браузера.\n",
    "\n",
    "Tensorboard умеет:\n",
    "\n",
    "1. Рисовать граф вычислений.\n",
    "2. Граф вычислений стоит посмотреть хотя бы для самопроверки, чтобы убедиться в том, что собралось и считается именно то, что планировалось, и при кодировании не допущено ошибок.\n",
    "3. Показывать статистику по переменным.\n",
    "4. Можно собирать вообще любую статистику.\n",
    "5. Есть средство для анализа многомерных данных (например, эмбеддингов). Для этого в дашборде встроены PCA и t-SNE, с которыми можно попробовать рассмотреть данные в 2 и 3 измерениях.\n",
    "6. Гистограммы. Можно строить гистограммы распределений выходов слоев сетей и поведения переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратная сторона медали — чтобы статистика попадала в дашборд, её нужно сохранять в логи (в формате protobuf) с помощью специального API. API не очень сложный, сгруппирован в **tf.summary**. Для сбора статистики нужно будет отдельно зарегистрировать интересующие переменные с помощью специальных функций и потом отдельно сохранить всё в лог."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_output:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.histogram(\"layer_output\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.summary.scalar(\"accuracy\", learning_rate)**\n",
    "\n",
    "Сохранять логи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs/nn_logs\", sess.graph) # for 1.0\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простого: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "merged = tf.summary.merge_all(key='summaries')\n",
    "if not os.path.exists('tensorboard_logs/'):\n",
    "    os.makedirs('tensorboard_logs/')\n",
    "my_writer = tf.summary.FileWriter('tensorboard_logs/', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию Tensorboard локально доступен по адресу 127.0.1.1:6006. **`tensorboard --logdir=path/to/log-directory`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache beam and tf tranform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF: https://github.com/tensorflow/transform\n",
    "* Beam: https://beam.apache.org/documentation/programming-guide/\n",
    "* Google about this: https://research.googleblog.com/2017/02/preprocessing-for-machine-learning-with.html?m=1\n",
    "\n",
    "**Apache Beam**.Это набор интерфейсов для создания data processing pipeline. Вы пишете программу с помощью этих интерфейсов, а потом запускаете ее на конкретном движке, будь это Apache Spark или Google Cloud DataFlow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam только для Python 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install tensorflow-transform\n",
    "pip install apache-beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tr.transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.Transform** библиотека для выполнения предварительной обработки данных с TensorFlow. Эта предварительная обработка принимает различные формы: от преобразования форматов, формирования словарей, к выполнению множества числовых операций, таких как нормализация.  \n",
    "\n",
    "Он позволяет комбинировать различные фреймворки обработки данных (Apache Beam). Удобно, что можно включать это в сам граф Tensorflow (можно увидеть сколько занимал препроцессинг данных, например). Или можно экспортировать граф обработки данных, чтобы препроцессинг и обучение модели были более менее независимыми. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intro transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то, что TF позволяет работать с батчами данных, для некоторых операций необходим полный проход по датасету.Например, нормализация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Определение функции Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем, как определить «preprocessing function», которая является логическим описанием pipline, который преобразует исходные данные в данные, которые будут использоваться для обучения модели ML. \n",
    "\n",
    "Набор данных - словарь столбцов, а функция предварительной обработки определяется двумя основными механизмами:\n",
    "1. **tf.map**: принимает определенную пользователем функцию, которая принимает и возвращает тензоры. Такая функция может использовать любую операцию TensorFlow для построения выходных тензоров (от входных). Остальные аргументы - столбцы, к которым применяется функция. Количество столбцов = количеству аргументов функции. Аналог `map` в Python. Каждая строка обрабатывается независимо друг от друга.\n",
    "2. **analyzers**: анализаторы являются функциями, которые принимают один или несколько колонок и ворзвращает некоторые сводные статистические данные для входного столбца или столбцов. Пример анализатора **tft.min**.\n",
    "\n",
    "Объединив анализаторы и tft.map пользователи могут гибко создавать pipline для преобразования данных. Следующая функция предварительной обработки преобразует каждый из трех колонок по-разному."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  x = inputs['x']\n",
    "  y = inputs['y']\n",
    "  s = inputs['s']\n",
    "  x_centered = tft.map(lambda x, mean: x - mean, x, tft.mean(x))\n",
    "  y_normalized = tft.scale_to_0_1(y)\n",
    "  s_integerized = tft.string_to_int(s)\n",
    "  x_centered_times_y_normalized = tft.map(lambda x, y: x * y,\n",
    "                                          x_centered, y_normalized)\n",
    "  return {\n",
    "      'x_centered': x_centered,\n",
    "      'y_normalized': y_normalized,\n",
    "      'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
    "      's_integerized': s_integerized\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* `x, y, s` входные столбцы\n",
    "* Первый новый столбец **x_centered**, строится путем составления `tft.map и tft.mean`. **tft.mean(x)** возвращает статистическую величину, представляющую среднее значение столбца x.\n",
    "* Второй новый столбец **y_normalized**, созданный таким же образом, но с использованием методы **tft.scale_to_0_1**.\n",
    "* Колонка **s_integerized** показывает пример работы со строками. В этом простом случае мы берем строку и переводим в целое число.\n",
    "* **x_centered_times_y_normalized** объединенная колонка.\n",
    "\n",
    "Типичные рабочий процесс пользователя tf.Transform будет построить функцию предварительной обработки, а затем включить это в больший pipline Beam (материализует данные для обучения).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хотя это не очевидно из приведенного выше примера, заданная пользовательская функция передается tft.map будет передаваться тензорами, представляющих батчи, а не отдельные экземпляры, так же, как будет происходить во время подготовки TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация Canonical Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.Transform** обеспечивает каноническую реализацию препроцессинга данных, которая запускает функцию предварительной обработки на Apache Beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = [\n",
    "    {'x': 1, 'y': 1, 's': 'hello'},\n",
    "    {'x': 2, 'y': 2, 's': 'world'},\n",
    "    {'x': 3, 'y': 3, 's': 'hello'}\n",
    "]\n",
    "\n",
    "raw_data_metadata = ...\n",
    "transformed_dataset, transform_fn = (\n",
    "    (raw_data, raw_data_metadata) | beam_impl.AnalyzeAndTransformDataset(\n",
    "        preprocessing_fn, tempfile.mkdtemp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Содержание **transformed_dataset** будет показано ниже, он содержать преобразованные столбцы в том же формате, что и исходные данные. В частности, значения s_integerized = `[0, 1, 0]`(эти значения зависят от того, как слова hello и world были нанесены на карту в целые числа, который является детерминированным). Для столбца x_centered мы вычитали среднее значение, поэтому значения столбца x,  `[1.0, 2.0, 3.0]` стали `[-1.0, 0.0, 1.0]`. Точно так же все остальные столбцы соответствуют их ожидаемым значениям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[{u's_integerized': 0,\n",
    "  u'x_centered': -1.0,\n",
    "  u'x_centered_times_y_normalized': -0.0,\n",
    "  u'y_normalized': 0.0},\n",
    " {u's_integerized': 1,\n",
    "  u'x_centered': 0.0,\n",
    "  u'x_centered_times_y_normalized': 0.0,\n",
    "  u'y_normalized': 0.5},\n",
    " {u's_integerized': 0,\n",
    "  u'x_centered': 1.0,\n",
    "  u'x_centered_times_y_normalized': 1.0,\n",
    "  u'y_normalized': 1.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Оба **raw_data** и **transformed_data** являются наборами данных. Другое возвращаемое значение, **transform_fn** является представлением преобразования, которое было сделано с данными (которые мы обсудим более подробно ниже).\n",
    "\n",
    "В самом деле, **AnalyzeAndTransformDataset** представляет собой композицию из двух фундаментальных преобразований , предусмотренных реализации, **AnalyzeDataset** и **TransformDataset**. То есть, эти два фрагмента кода ниже эквивалентны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformed_data, transform_fn = (\n",
    "    my_data | AnalyzeAndTransformDataset(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform_fn = my_data | AnalyzeDataset(preprocessing_fn)\n",
    "transformed_data = (my_data, transform_fn) | TransformDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**transform_fn** является чистой функцией, которая представляет собой операцию, применяемая к каждой строке набора данных. В частности, все значения анализатора уже вычислены и рассматриваются как константы. В нашем примере, transform_fn будет содержать в качестве констант среднее значение столбца x, min/max колонны y и словарь, используемый для отображения строки в целые числа.\n",
    "\n",
    "\n",
    "Ключевая особенность tf.Transform является то , что transform_fnпредставляет собой map над rows (чистая функция, применяемая к каждой строке отдельно). Все вычисления с участием агрегирования строк делается **AnalyzeDataset**. Кроме того, transform_fn представляются в виде TensorFlow Graph, что означает, что он может быть встроен в обслуживающий граф.\n",
    "\n",
    "Google предлагает **AnalyzeAndTransformDataset** для того, чтобы обеспечить оптимизацию. Это аналоги scikit-learn, который обеспечивает fit, transformи fit_transform методы для препроцессоров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Форматы данных и схемы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы упустили код для **raw_data_metadata**. В метаданных храниться схема, которая определяет расположение данных таким образом, что она может быть записана и прочитана в различные форматы (описание ниже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema({\n",
    "    's': dataset_schema.ColumnSchema(tf.string, [],\n",
    "        dataset_schema.FixedColumnRepresentation()),\n",
    "    'y': dataset_schema.ColumnSchema(tf.float32, [],\n",
    "        dataset_schema.FixedColumnRepresentation()),\n",
    "    'x': dataset_schema.ColumnSchema(tf.float32, [],\n",
    "        dataset_schema.FixedColumnRepresentation())\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **dataset_schema.Schema** класс представляет собой оболочку вокруг Dict из **dataset_schema.ColumnSchema**. Каждый ключ в cловаре описывает имя тензора, ColumnSchema описывает вид тензора и как он представлен в памяти или на диске.\n",
    "2. Первый аргумент **ColumnSchema** определяет, Domain который включает в себя тип данных и некоторые детали, такие как диапазоны. В нашем случае мы указали только тип данных.\n",
    "3. Второй аргумент содержит список **axis** объектов, которые описывают форму тензора. В нашем случае нет осей, так как это скаляры (rank 0 tensor).\n",
    "4. Третий аргумент является представлением данных. Есть три вида представления: \n",
    "    *  **FixedColumnRepresentation** является представлением колонки с фиксированным, известного размером.\n",
    "    * **ListColumnRepresentation** представляет колонки с разным размером\n",
    "    * **SparseColumnRepresentation** колонки с фиксированным размером (sparse  представление).\n",
    "    * подробнее [tf_metadata/dataset_schema.py](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_schema.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### IO with BEAM implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Строим схему данных\n",
    "2. `csv_coder.CsvCoder`. Используем схему для чтения данных .csv. `ordered_columns` содержит названия всех колонок в порядке их появления в файле (сама схема не содержит эту информацию). \n",
    "3. Далее производим чтение из файла >> производим Map - декодирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "converter = csv_coder.CsvCoder(ordered_columns, raw_data_schema)\n",
    "\n",
    "raw_data = (\n",
    "    p\n",
    "    | 'ReadTrainData' >> textio.ReadFromText(train_data_file)\n",
    "    | ...\n",
    "    | 'DecodeTrainData' >> beam.Map(converter.decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предварительная обработка затем переходит аналогично предыдущему примеру, за исключением того, что мы программно генерировали функцию предварительной обработки вместо того, чтобы вручную указать каждый столбец. Функция предварительной обработки показана ниже. NUMERICAL_COLUMNS и CATEGORICAL_COLUMNS списки, содержащие имена числовых и категориальных столбцов соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
    "  outputs = {}\n",
    "\n",
    "  # Scale numeric columns to have range [0, 1].\n",
    "  for key in NUMERIC_COLUMNS:\n",
    "    outputs[key] = tft.scale_to_0_1(inputs[key])\n",
    "\n",
    "  # For all categorical columns except the label column, we use\n",
    "  # tft.string_to_int which computes the set of unique values and uses this\n",
    "  # to convert the strings to indices.\n",
    "  for key in CATEGORICAL_COLUMNS:\n",
    "    outputs[key] = tft.string_to_int(inputs[key])\n",
    "\n",
    "  # For the label column we provide the mapping from string to index.\n",
    "  def convert_label(label):\n",
    "    table = lookup.string_to_index_table_from_tensor(['>50K', '<=50K'])\n",
    "    return table.lookup(label)\n",
    "  outputs[LABEL_COLUMN] = tft.map(convert_label, inputs[LABEL_COLUMN])\n",
    "\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**raw_data** Переменная представляет собой PCollection содержащий данные в том же формате, что и список raw_data из предыдущего примера, а также использование из AnalyzeAndTransformDataset преобразования одно и то же. Обратите внимание , что схема используется в двух местах: чтение данных из файла CSV, а также в качестве вклада AnalyzeAndTransformDataset. Это происходит потому , что как формат CSV и формат , в памяти должны быть соединены со схемой для того , чтобы интерпретировать их как тензоры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
